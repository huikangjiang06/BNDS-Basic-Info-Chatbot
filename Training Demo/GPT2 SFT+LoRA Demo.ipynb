{"cells":[{"id":"53cde4d1-8784-4728-ac05-9b4d5d6ba4c3","cell_type":"code","source":"!unset LD_LIBRARY_PATH","metadata":{},"execution_count":" "},{"id":"40f38e84-d871-44c4-89e4-964067241769","cell_type":"code","source":"!pip show torch","metadata":{},"execution_count":" "},{"id":"e9809288-a911-46aa-8551-d6dd3f2681d7","cell_type":"code","source":"!pip show cudatoolkit","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"9c9cf166-69df-41b9-b3f9-37f314fa4d33","metadata":{},"source":"import os\nimport math\nimport random\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#from sklearn import datasets, linear_model, metrics, model_selection, preprocessing\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset"},{"cell_type":"markdown","id":"14a0f180-1f1a-4005-802e-254c5508736d","metadata":{},"source":"### Implementing Attention"},{"cell_type":"code","execution_count":" ","id":"138bef64-0efe-4c67-b4c1-2cd67e3965cb","metadata":{},"source":"class SelfAttention(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))        ### W: [d_in, d_out]\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n    def forward(self, x):                                           ### x : [batch_size, seq_length, d_in]\n        keys = x @ self.W_key                                       ### keys: [batch_size, seq_length, d_out], @: 矩阵乘法\n        queries = x @ self.W_query\n        values = x @ self.W_value\n        attn_scores = torch.bmm(queries, keys.transpose(1, 2))      ### 计算注意力分数: [batch_size, seq_length, seq_length]\n        attn_weights = torch.softmax(attn_scores \n                                     / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values                         ### 输出： [batch_size, seq_length, output_dimension]\n        return context_vec"},{"cell_type":"code","execution_count":" ","id":"1badf962-9492-442e-a2a4-83c6d572f861","metadata":{},"source":"class CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)        ### 没有偏置的线性层，[d_in, d_out]的矩阵，跟Parameter效果一样\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)                          ### 设置Dropout概率\n        self.register_buffer(                                       ### 设置一个上三角矩阵\n           'mask',\n           torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n        \n    def forward(self, q, k, v):\n        b, num_tokens, d_in = q.shape\n        keys = self.W_key(k)\n        queries = self.W_query(q)\n        values = self.W_value(v)\n        attn_scores = queries @ keys.transpose(1, 2) \n        attn_scores.masked_fill_(                                   ### 取上三角矩阵的左上部分，然后把1换成-inf\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)   \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1) ### -inf经过softmax得0\n        attn_weights = self.dropout(attn_weights)\n        context_vec = torch.matmul(attn_weights,values)\n        return context_vec"},{"cell_type":"code","execution_count":" ","id":"a913fe3b-1bca-43f8-9122-ce426b946e0e","metadata":{},"source":"class MultiheadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length,dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(                                 ### 重复多头\n            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n            for _ in range(num_heads)\n        )\n    def forward(self, q, k, v):\n        return torch.cat([head(q, k, v) for head in self.heads], dim=-1)  ### 多头注意力的输出是拼接，不是相加，保留各头自己的特征，然后传给线性层"},{"cell_type":"markdown","id":"793d1a8d-e311-4f38-852b-f9735f0c9e6e","metadata":{},"source":"### Implementing Transformer"},{"cell_type":"code","execution_count":" ","id":"6c6be980-e5c0-4878-bcc4-573becc132c6","metadata":{},"source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_in, context_length, num_heads, dropout, qkv_bias=False):\n        super().__init__()\n        d_out = d_in // num_heads\n        self.att = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n        self.norm1 = nn.LayerNorm(d_in)\n        self.norm2 = nn.LayerNorm(d_in)\n        self.drop_resid = nn.Dropout(dropout)\n        self.ff = nn.Sequential(\n            nn.Linear(d_in,d_in*4),\n            nn.ReLU(),\n            nn.Linear(d_in*4,d_in)\n        )\n    def forward(self, x):\n        value = x\n        x = self.att(x, x, x)\n        x = self.drop_resid(x)\n        x = x + value\n        x = self.norm1(x)\n        value = x\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + value\n        x = self.norm2(x)\n        return x"},{"cell_type":"code","execution_count":" ","id":"6b7b70eb-bfdd-4f26-a51e-25d1d005bfa6","metadata":{},"source":"class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_in, context_length, num_heads, dropout, qkv_bias=False):\n        super().__init__()\n        d_out = int(d_in/num_heads)\n        self.att1 = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n        self.att2 = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n        self.norm1 = nn.LayerNorm(d_in)\n        self.norm2 = nn.LayerNorm(d_in)\n        self.norm3 = nn.LayerNorm(d_in)\n        self.drop_resid = nn.Dropout(dropout)\n        self.ff = nn.Sequential(\n            nn.Linear(d_in, d_in*4),\n            nn.ReLU(),\n            nn.Linear(d_in*4, d_in)\n        )\n    def forward(self, x, enc_output):\n        value = x\n        x = self.att1(x, x, x)\n        x = self.drop_resid(x)\n        x = x + value\n        x = self.norm1(x)\n        value = x\n        x = self.att2(x, enc_output, enc_output) \n        x = self.drop_resid(x)\n        x = x + value\n        x = self.norm2(x)\n        value = x\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + value\n        x = self.norm3(x)\n        return x"},{"id":"80edaff1-b214-4f98-b706-ff128d1245cd","cell_type":"markdown","source":"### Implementing GPT-2","metadata":{}},{"id":"a17c5898-b219-4580-b073-2a97bcc817ab","cell_type":"markdown","source":"**GPT2结构**\\\n词表大小：50257\\\n词嵌入层：（50257,768）词表索引转换为768维度向量\\\n位置索引：（1024,768）最大token限制1024，每一个token一个768维位置索引\\\nDropout：对embedding做dropout，p=0.1\\\nTransformer Decoder块12个\\\n层归一化 768\\\nLinear （768，768×3）\\\n将Linear层的输出分割成3个部分，作为注意力层的三个输入\\\nAttention (768,768/12) 12个注意力头\\\n拼接12个注意力头 12×768/12 ——> 768\\\n线性层（768,768）\\\nDropout(0.1)\\\nresidual位置，加入初始输入值\\\n层归一化\\\n线性层（768,768×3）\\\nGELU()\\\n线性层（768×3,768）\\\nDropout(0.1)\\\n到此decoder块结束\\\n在所有decoder块之后有一个层归一化、一个线性层、接softmax输出\n\n\n","metadata":{}},{"id":"c57f017f-b9cd-40bb-bfca-37e4678a426b","cell_type":"code","source":"class MoE_Expert(nn.Module):\n    def __init__(self,d_in,dropout=0.1):\n        super().__init__()\n        self.up=nn.Linear(d_in, 4*d_in, bias=False)\n        self.gate=nn.Linear(d_in, 4*d_in, bias=False)\n        self.down=nn.Linear(4*d_in, d_in, bias=False)\n        self.dropout=nn.Dropout(dropout)\n        self.act_fn=nn.GELU()\n\n    def forward(self,x):\n        x = self.act_fn(self.gate(x)) * self.up(x)\n        x = self.down(x)\n        x = self.dropout(x)\n        return x","metadata":{},"execution_count":" "},{"id":"058e3b68-1fd1-4552-af7c-860dbc800301","cell_type":"code","source":"class MoE_Router(nn.Module):\n    def __init__(self,d_in,num_experts,topk):\n        super().__init__()\n        self.topk = topk\n        self.ln = nn.Linear(d_in, num_experts)\n\n    def forward(self,x):\n        x = self.ln(x)\n        logits, indices = x.topk(self.topk, dim=-1)\n        logits = torch.full_like(x, float('-inf')).scatter_(-1, indices, logits)\n        weight = nn.functional.softmax(logits, dim=-1)\n        return weight, indices","metadata":{},"execution_count":" "},{"id":"1c5647e5-9a60-4db4-a803-5b8524be4923","cell_type":"code","source":"class MoE(nn.Module):\n    def __init__(self,d_in,num_expers,topk,dropout=0.1):\n        super().__init__()\n        self.router=MoE_Router(d_in,num_expers,topk)\n        self.experts=nn.ModuleList(MoE_Expert(d_in,dropout) for _ in range(num_expers))\n\n    def forward(self,x):\n        weight, indices = self.router(x)\n        out = torch.zeros_like(x)\n        x_flat = x.reshape(-1, x.shape[-1])\n        weight_flat = weight.reshape(-1, weight.shape[-1])\n        for i, expert in enumerate(self.experts):\n            mask = (indices == i).any(dim=-1)\n            mask_flat = mask.reshape(-1)\n            if mask.any():\n                outi = expert(x_flat[mask_flat])\n                score = weight_flat[mask_flat, i].unsqueeze(1)\n                outi = outi * score\n                out[mask] += outi.squeeze(1)\n        return out","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"1e58532c-29ce-4230-8b27-4ac8ea962de7","metadata":{},"source":"class GPT2Layer(nn.Module):\n    def __init__(self, d_in, context_length=1024, num_heads=12, use_moe=True, num_experts=4, topk=2, dropout=0.1, qkv_bias=False):\n        super().__init__()\n        self.d_in = d_in\n        d_out = int(d_in/num_heads)\n        \n        self.drop_emb = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_in)\n        self.linear1 = nn.Linear(d_in,d_in*3)\n        self.att = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n        self.drop_resid = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_in,d_in)\n        self.norm2 = nn.LayerNorm(d_in)\n        \n        if use_moe:\n            self.ff = MoE(d_in, num_experts, topk, dropout)\n        else:\n            self.ff = nn.Sequential(\n                nn.Linear(d_in, d_in*4),\n                nn.GELU(),\n                nn.Linear(d_in*4, d_in),\n                nn.Dropout(dropout)\n            )\n        \n    def forward(self, x):\n        value = x\n        x = self.drop_emb(x)\n        x = self.norm1(x)\n        x = self.linear1(x)\n        q, k, v = x.split(self.d_in, dim=-1)\n        x = self.att(q, k, v)\n        x = self.linear2(x)\n        x = self.drop_resid(x)\n        x = x + value\n        \n        value = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = x + value\n        return x"},{"id":"10bf5222-55f6-4564-a93d-4b97c39bdb84","cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1024):\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.max_len = max_len\n\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n        \n        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        pe = self.pe[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)\n        x = x + pe\n        return x","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"9f765201-e3bd-4676-99bc-037b31f62750","metadata":{},"source":"class GPT2Model(nn.Module):\n    def __init__(self, num_layers=12, vocab_size=50257, hidden_size=768, context_length=1024, num_heads=12):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.emb = nn.Embedding(vocab_size, hidden_size)\n        self.layers = nn.Sequential(\n            *[GPT2Layer(hidden_size, context_length, num_heads, dropout=0, qkv_bias=False) \n             for _ in range(num_layers)]\n        )\n        self.norm = nn.LayerNorm(hidden_size)\n        self.ln=nn.Linear(hidden_size,vocab_size)\n        self.pe = PositionalEncoding(d_model=hidden_size,max_len=context_length)\n        \n    def forward(self, x):\n        x = self.emb(x)\n        x = self.pe(x)\n        x = self.layers(x)\n        x = self.norm(x)\n        x = self.ln(x)\n        return x"},{"id":"c04e6e47-f976-4f38-bbcb-dc16009fa026","cell_type":"markdown","source":"### Training GPT2","metadata":{}},{"id":"037c15fd-1526-4032-bf24-890ff0f3c3a7","cell_type":"code","source":"!pip install snowballstemmer","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"fe768bc1-205a-4c94-a7c3-c11ef0337a9c","metadata":{},"source":"import tokenize\nimport snowballstemmer\nimport collections\n\ndef load_data(path='/bohr/wikipedia-2l1k/v1/wikisent2.txt',max_len=50,vocab_size=10000):\n    texts=[]\n    line=[]\n    lens=[]\n    stemmer=snowballstemmer.stemmer('english')\n    counter=collections.Counter()\n    c=0               # 防止测试时等太久所以加上，之后可以删掉\n    with open(path,'rb') as f:\n        lines=tokenize.tokenize(f.readline)\n        for s in lines:\n            word=s.string.lower()\n            if word == 'utf-8':\n                continue\n            if word == '\\n':\n                texts.append(line)\n                lens.append(len(line))\n                line=[]\n                c+=1\n            else:\n                word=stemmer.stemWord(word)\n                line.append(word)\n                counter.update([word])\n            if c>=10000:\n                break\n    plt.hist(lens,bins=20)\n    str2idx={str:idx+2 for idx,(str,_) in enumerate(counter.most_common(vocab_size))}\n    str2idx['<UNK>'],str2idx['<PAD>']=0,1\n    idx2str={idx:str for str,idx in str2idx.items()}\n    texts=[[str2idx[str] if str in str2idx else str2idx['<UNK>'] for str in line] for line in texts]\n    texts=[line[:max_len] if len(line)>=max_len else line+(max_len-len(line))*[str2idx['<PAD>']] for line in texts]\n    texts=torch.tensor(texts,dtype=torch.long)\n    print(texts.shape)\n    return str2idx,idx2str,texts"},{"id":"52bb5deb-cae5-4922-be58-93b630749a8c","cell_type":"code","source":"# Hyperparameters\nNUM_LAYERS=12\nVOCAB_SIZE=10000\nHIDDEN_SIZE=768\nCONTEXT_LENGTH=50\nNUM_HEADS=12\nBATCH_SIZE=16\nMAX_LEN=50\nPATH='/bohr/wikipedia-2l1k/v1/wikisent2.txt'","metadata":{},"execution_count":" "},{"id":"788d3cdd-c091-47e9-a919-d1e1d722c463","cell_type":"code","source":"model=GPT2Model(NUM_LAYERS, VOCAB_SIZE, HIDDEN_SIZE, CONTEXT_LENGTH, NUM_HEADS).cuda()","metadata":{},"execution_count":" "},{"id":"77d46a2e-4185-4b53-a704-f66e045f39bf","cell_type":"code","source":"str2idx,idx2str,texts=load_data(PATH, MAX_LEN, VOCAB_SIZE)","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"71b493ba-7fc0-4ff3-9737-6b3c25a03bec","metadata":{},"source":"dataset=TensorDataset(texts)\ndataloader=DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\nprint(dataset[0])"},{"id":"f5214c44-766b-49d9-b51e-be9b03ca287d","cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{},"execution_count":" "},{"cell_type":"code","execution_count":" ","id":"402ed684-0adf-46b5-b9e4-6e92edbf15aa","metadata":{},"source":"from tqdm import tqdm\noptim_fn=torch.optim.Adam(model.parameters(),lr=0.01)\nloss_fn=nn.CrossEntropyLoss()\nfor epoch in range(10):\n    losses=0\n    accs=0\n    batch_counter=0\n    pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}')\n    for text in pbar:\n        text=text[0].cuda()\n        x,y=text[:,:-1],text[:,1:]\n        y_pred= model(x)\n        loss=loss_fn(y_pred.transpose(1,2),y)\n        optim_fn.zero_grad()\n        loss.backward()\n        optim_fn.step()\n        losses+=loss.item()\n        _,y_pred=torch.max(y_pred,dim=-1)\n        accs+=(y_pred==y).to(torch.float).mean()\n        batch_counter+=1\n    losses/=batch_counter\n    accs/=batch_counter\n    print(f'loss: {losses} acc: {accs}')"},{"id":"50b0f1ee-057e-466b-93c2-e593107ccf13","cell_type":"code","source":"","metadata":{},"execution_count":null},{"id":"c149bd23-e653-4d45-a006-560b47329670","cell_type":"code","source":"","metadata":{},"execution_count":null},{"id":"23ff1857-4853-4ede-b545-eb86592ff094","cell_type":"markdown","source":"### Full Parameter Learning with Transformers and Tokenizers","metadata":{}},{"id":"11cb4c05-a161-481c-9da4-86e915cbbcbd","cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom datasets import load_dataset, Dataset\n\nmodel = GPT2LMHeadModel.from_pretrained(\"/personal/gpt2/\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"/personal/gpt2/\")\n\n'''\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\") ### 从huggingface直接下载数据\n直接下载时的数据结构：\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n        num_rows: 10000\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n        num_rows: 3000\n    })\n})\n'''\n\n### 从本地的.txt生成数据\nfile_path = \"/personal/wikisent2.txt\"\nwith open(file_path, 'r', encoding='utf-8') as f:\n    lines = [line.strip() for line in f if line.strip()]\n    dataset = Dataset.from_dict({\"text\": lines})\n\n ### 50个测试\ndataset = dataset.select(range(50))\n\ndef tokenize_function(dataset):\n    return tokenizer(\n        dataset[\"text\"],\n        truncation=True,\n        max_length=128, \n        padding=\"max_length\"\n    )\n\n### 因模型而异，gpt2没有预设padding的编码\ntokenizer.pad_token = tokenizer.eos_token\n\n'''\n经过dataset.map之后的数据结构：\nDataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 50\n})\n自动生成词表索引列和注意力掩码列，其中attention_mask 1 表示真实值，0 表示填充值。模型不会计算0位置的损失。可以手动更改attention_mask。\n'''\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n### 删去空数据\ntokenized_dataset = tokenized_dataset.filter(lambda example: len(example['input_ids']) > 0)\n\n'''\nGPT-2期望的输入数据：\n{\n    \"input_ids\": torch.tensor([[50256, 1234, 5678, 9012, 50256]]),\n    \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1]]),              \n    \"labels\": torch.tensor([[1234, 5678, 9012, 50256, -100]]) \n}\n'''\n### 加入labels列，和input一样，模型自动自回归\ntokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n\n### train test split\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)\ntrain_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\n\ntraining_args = TrainingArguments(\n    output_dir=\"./model\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n\n\ndevice = 'cuda'\nmodel.to(device)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n### training\ntrainer.train()\n\ntrainer.save_model(\"./model\")\ntokenizer.save_pretrained(\"./model\")\n\n### 预测\nprompt = \"Jack went to school yesterday and \"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\noutput = model.generate(\n    inputs.input_ids,\n    max_length=50,\n    num_return_sequences=3,\n    num_beams=3,\n    do_sample=True,\n    temperature=3.0,\n    pad_token_id=tokenizer.eos_token_id,\n)\n\nfor i, sample in enumerate(output):\n    print(f\"{i+1}: {tokenizer.decode(sample, skip_special_tokens=True)}\")","metadata":{},"execution_count":null},{"id":"53253839-80e9-465d-a34b-25c27226b725","cell_type":"markdown","source":"### Supervised Fine Tuning on Q/A Pairs","metadata":{}},{"id":"dfa2344e-7f71-4458-8a29-e308436fd43d","cell_type":"code","source":"### 假设现在本地有准备好的问答对数据，用们微调GPT-2来让他学习这些问答对\n\nfrom datasets import Dataset\n\ndef preprocess_function(examples, tokenizer, max_length=128):\n    inputs = examples[\"prompt\"]\n    targets = examples[\"response\"]\n    \n    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=\"max_length\")\n    \n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ndef train(model, tokenzier, dataset, output_dir=\"./sft_model\"):\n\n    tokenized_dataset = dataset.map(\n        lambda dataset: preprocess_function(dataset, tokenizer),\n        batched=True,\n        remove_columns=dataset.column_names\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=4,\n        num_train_epochs=3,\n        save_steps=500,\n        logging_steps=100,\n        evaluation_strategy=\"no\",\n        overwrite_output_dir=True,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n    \n    trainer.train()","metadata":{},"execution_count":null},{"id":"48b85176-3ce4-4537-a8d5-add381c2d8b2","cell_type":"code","source":"file_path = \"/Users/jianggh/Desktop/IOAI/轻量级十一学校信息问答语言模型/问答对.xlsx\"\ndf = pd.read_excel(file_path, header=None, names=['prompt', 'response'])\ndataset = Dataset.from_pandas(df)\n\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\ntrain(model,tokenzier,dataset)","metadata":{},"execution_count":null},{"id":"7e0c5c20-735d-4211-ac7f-7d7c82b116a4","cell_type":"markdown","source":"### LoRA Fine Tuning with PyTorch\n","metadata":{}},{"id":"2eb50819-38db-44be-9066-d63ce5cf082c","cell_type":"code","source":"### import peft, PEFT是huggingface提供的进行微调的库，包括lora，但是目前比赛并没有提供这个PEFT，所以我们用PyTorch来实现lora\n\n### 定义lora网络结构\nclass LoRA(nn.Module):\n    def __init__(self, in_features, out_features, rank=8, alpha=16):\n        super().__init__()\n        self.rank = rank  ### LoRA的秩（rank），控制低秩矩阵的大小\n        self.scaling = alpha ### 用来控制lora层的scaling参数\n        self.A = nn.Linear(in_features, rank, bias=False)  ### 低秩矩阵A\n        self.B = nn.Linear(rank, out_features, bias=False)  ### 低秩矩阵B\n        \n        self.A.weight.data.normal_(mean=0.0, std=0.02) ### 矩阵参数初始化\n        self.B.weight.data.zero_()\n\n    def forward(self, x):\n        return self.B(self.A(x)) * self.scaling\n\n### 在输入模型中加入lora层\ndef apply_lora(model, \n               layer_names=[\"query\", \"key\", \"value\", \"dense\"], ### 选择需要lora的原始module\n               rank=8, \n               alpha=1/16):\n    for name, module in model.named_modules(): ### 遍历model中的所有module\n        if isinstance(module, nn.Linear) and any(key in name for key in layer_names): ### 选择需要lora的原始module，可以更改这个条件\n            \n            lora = LoRA(module.in_features, module.out_features, rank, alpha).to(model.device)\n            setattr(module, \"lora\", lora) ### 将lora实例设置为module的属性，并命名为\"lora\"，以便后续识别\n            original_forward = module.forward\n\n            for param in module.parameters(): ### 冻结原始module的权重\n                param.requires_grad = False\n\n            def forward_with_lora(x, layer1=original_forward, layer2=lora): ### 将原来的foward函数替代\n                return layer1(x) + layer2(x)\n\n            module.forward = forward_with_lora\n\n### 只保存lora层\n### 提交比赛的时候也可以直接torch.save(model.state_dict())\ndef save_lora(model, path):\n    state_dict = {}\n    for name, module in model.named_modules():\n        if hasattr(module, 'lora'): ### 此时判断module是否是lora，只保存lora\n            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()} ### 此时遍历，k是参数名，v是参数值，保存\"{name}.lora.{k}\"确定参数位置\n            state_dict.update(lora_state)\n    torch.save(state_dict, path)\n\ndef load_lora(model, path):\n    state_dict = torch.load(path, map_location=model.device)\n    for name, module in model.named_modules():\n        if hasattr(module, 'lora'):\n            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}\n            module.lora.load_state_dict(lora_state)","metadata":{},"execution_count":null},{"id":"8d9a47e3-acac-4dc2-a69a-b616759e26c9","cell_type":"code","source":"### 假设我们要完成一个分类任务，我们可以这样添加一个线性层在模型后面\nclass ClassificationModel(nn.Module):\n    def __init__(self, model, num_labels):\n        super().__init__()\n        self.model = model\n        self.classifier= nn.Linear(model.config.hidden_size, num_labels)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.backbone(input_ids, attention_mask=attention_mask)\n        output = outputs.last_hidden_state[:, 0]\n        return self.classifier(output)\n\n\nclass TweetClassificationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        label = self.data[idx][\"target\"]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n\ndef train(model, train_loader, optimizer, device, epochs=3):\n    model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    \n    for epoch in range(epochs):\n        model.train()\n\n        for batch in train_loader:\n            inputs = batch[\"input_ids\"].to(device)\n            masks = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs, masks)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()","metadata":{},"execution_count":null},{"id":"16b593b3-53e0-426b-9828-3523a9fe8cf0","cell_type":"code","source":"model_name = \"/personal/gpt2/\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2) ### 使用ForSequenceClassification而不是ForCausalLM，这样子会自动添加线性层\n### 亦可 model = ClassificationModel(model,2)\napply_lora(model)\n\n### 某一个序列分类问题\ndataset = datasets.load_dataset(\"mehdiiraqui/twitter_disaster\")\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"val\"]\n\ntrain_dataset = TweetClassificationDataset(train_data, tokenizer, max_length=128)\nval_dataset = TweetClassificationDataset(val_data, tokenizer, max_length=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n### 确保原始modules不参与梯度计算\nfor name, param in model.named_parameters():\n    if 'lora' not in name and \"classifier\" not in name:\n        param.requires_grad = False\nlora_params = []\nfor name, param in model.named_parameters():\n    if 'lora' in name or \"classifier\" in name:\n        lora_params.append(param)\n        \n### 只需要给optimizer喂lora层\noptimizer = optim.AdamW(lora_params, lr=5e-5)\n\ntrain(model, train_loader, optimizer, device='cuda', epochs=3)","metadata":{},"execution_count":null},{"id":"11f72b3a-8854-4374-bf05-26c3be359ca3","cell_type":"markdown","source":"### Direct Preference Optimization","metadata":{}},{"id":"a08a96db-822a-4c69-a0c3-74fefb35b5d3","cell_type":"code","source":"","metadata":{},"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}